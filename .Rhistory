# This script reads in the raw .csv files that were produced by Psychopy. These
# files have terrible formatting and include a lot of data that is not relevant
# for our analysis. We import these files and transform them into a data frame
# with one line per trial.
rm(list = ls(all.names = TRUE))
library(pacman)
p_load(dplyr, tidyr, stringr, readr)
# Define all variable decisions of this analysis.
vars                     <- list()
vars$exp_name            <- "scenecat_2.0"
vars$min_logfile_kbsize <- 50 # some logfiles do not include all metadata (probably because the experiment was immediately interrupted). this makes the code crash. we need to exclude these files.
# We need to code where a given dataset was collected. This is not coded in
# thePsychopy logfiles, but we can infer it from certain columns that are unique
# to the Prague/Muenster version.
vars$place_strings <- list(
"vpcode" = "Muenster",
"email.for.compensation" = "Prague",
"If.you.are.participating.for.credits.under.the.LABELS.lab..fill.in.your.email.address.." = "Prague"
)
# The German version of this experiment has some columns named in German. Wie
# need to rename these columns for consistency with the English version.
vars$column_renames <- c("Alter"       = "age",
"Geschlecht"  = "gender",
"Haendigkeit" = "handedness")
vars$columns_of_interest <- c("participant", "age", "gender", "handed", "place",
"task",
"target_cat", "cond_cat", "cat_key", "cat_corr", "cat_rt",
"cond_mem", "mem_response", "mem_perform",
"typicality", "conceptual", "perceptual",  "category", "stimulus"
)
# Where is all this stuff located? My folder structure is:
# 2024_scenecat2.0
# ├───code
# │   └───scenecat2.0_analysis
# │       └───functions
# └───data
#         ├───data_muenster
#         ├───data_muenster_pilot
#         └───data_prague
dirs <- list()
dirs$main       <- "C:/Users/nbusch/sciebo_box_projects/Projects/2024_scenecat2.0/"
dirs$functions  <- paste0(dirs$main, "/code/scenecat2.0_analysis/functions/")
dirs$data       <- paste0(dirs$main, "/data/")
# dirs$logfiles   <- paste0(dirs$data, "/data_muenster_pilot/")
# dirs$logfiles   <- paste0(dirs$data, "/data_muenster/")
dirs$logfiles   <- paste0(dirs$data, "/data_prague/")
# Get a list of logfiles to process.
source(file.path(dirs$functions, "fn_list_logfiles.R"))
logfile_list <- fn_list_logfiles(dirs, vars, "SceneCat_2023")
# Get an overview of our datasets.
source(file.path(dirs$functions, "fn_logfile_overview.R"))
logfile_overview <- fn_logfile_overview(logfile_list, vars, dirs)
# Read all logfiles and append the raw data.
source(file.path(dirs$functions, "fn_import_read_logfiles.R"))
raw_data <- fn_import_read_logfiles(logfile_list, vars)
# Get a list of logfiles to process.
source(file.path(dirs$functions, "fn_list_logfiles.R"))
logfile_list <- fn_list_logfiles(dirs, vars, "SceneCat_2023")
# Get an overview of our datasets.
source(file.path(dirs$functions, "fn_logfile_overview.R"))
logfile_overview <- fn_logfile_overview(logfile_list, vars, dirs)
summary_data <- data.frame()
# Loop over all files in the list
index <- 1
for (file_path in logfile_list) {
# Read the file
data <- read.csv(file_path)
# Check where the data where collected.
source(file.path(dirs$functions, "fn_check_logfile_place.R"))
place <- fn_check_logfile_place(data, vars)
source(file.path(dirs$functions, "fn_german_to_english.R"))
data <- fn_german_to_english(data, vars$column_renames)
# Makes sure that demographic variables are not empty.
variables <- c("age", "gender", "handedness", "email.for.compensation")
# Create a named list of the variables with values or NA
result <- lapply(variables, function(var) {
if (!is.null(data[[var]]) && length(unique(data[[var]])) == 1) {
unique(data[[var]])
} else {
NA
}
})
# Extract metadata
ID          <- unique(data$participant)
Age         <- unique(data$age)
Gender      <- unique(data$gender)
Handedness  <- unique(data$handedness)
Date        <- unique(data$date)
# Process the subject codes, depending on the place where the data where
# collected. This is important because we used slightly different version
# because our institutes had different ways to compensate participants.
if (place == "Muenster") {
# Get the value of vpcode from the last row, ensuring it's a single unique value
SubjectCode <- data$vpcode[which.max(seq_along(data$vpcode))]
if (is.na(SubjectCode) || SubjectCode == "" || length(SubjectCode) != 1) {
SubjectCode <- NA # Assign NA if vpcode is empty or not a single unique value
}
} else if (place == "Prague") {
# Get the first non-empty value of email.for.compensation
SubjectCode <- unique(data$email.for.compensation)
SubjectCode <- SubjectCode[!is.na(SubjectCode) & SubjectCode != ""]
if (length(SubjectCode) == 1) {
SubjectCode <- SubjectCode[1] # Keep as a single unique value
} else {
SubjectCode <- NA # Assign NA if there are no valid values or multiple conflicting values
}
} else {
# Handle cases where place is neither "muenster" nor "prague"
SubjectCode <- NA
}
# Ensure date is properly parsed
date_parsed <- lubridate::ymd_hms(Date)
day  <- format(date_parsed, "%Y-%m-%d")
time <- format(date_parsed, "%H:%M:%S")
# Count rows for memory and categorization tasks
nMemTrials <- sum(data$task == "memory", na.rm = TRUE)
nCatTrials <- sum(data$task == "categorization", na.rm = TRUE)
nTotTrials <- nMemTrials + nCatTrials
# Create a row summarizing the data
summary_row <- data.frame(
Index        = index,
ID           = ID,
SubjectCode  = SubjectCode,
Age          = Age,
Gender       = Gender,
Handedness   = Handedness,
Place        = place,
Day          = day,
Time         = time,
nMemTrials   = nMemTrials,
nCatTrials   = nCatTrials,
nTotTrials   = nTotTrials,
Filename     = basename(file_path), # Add the filename of the CSV
stringsAsFactors = FALSE
)
# Append the row to the summary data
summary_data <- rbind(summary_data, summary_row)
index <- index + 1
}
# Write output file.
require(openxlsx)  # Make sure to load the openxlsx package
overview_name <- paste0(vars$exp_name, "_overview.xlsx")
cat(sprintf("Saving logfile overview to %s\n", overview_name))
# Get a list of logfiles to process.
source(file.path(dirs$functions, "fn_list_logfiles.R"))
logfile_list <- fn_list_logfiles(dirs, vars, "SceneCat_2023")
# Get an overview of our datasets.
source(file.path(dirs$functions, "fn_logfile_overview.R"))
logfile_overview <- fn_logfile_overview(logfile_list, vars, dirs)
# Extract email addresses from the data frame
SubjectCode <- unique(na.omit(str_extract(as.matrix(data), email_regex)))
# Define a regex for email addresses
email_regex <- "[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}"
# Extract email addresses from the data frame
SubjectCode <- unique(na.omit(str_extract(as.matrix(data), email_regex)))
summary_data <- data.frame()
# Loop over all files in the list
index <- 1
for (file_path in logfile_list) {
# Read the file
data <- read.csv(file_path)
# Check where the data where collected.
source(file.path(dirs$functions, "fn_check_logfile_place.R"))
place <- fn_check_logfile_place(data, vars)
source(file.path(dirs$functions, "fn_german_to_english.R"))
data <- fn_german_to_english(data, vars$column_renames)
# Makes sure that demographic variables are not empty.
variables <- c("age", "gender", "handedness", "email.for.compensation")
# Create a named list of the variables with values or NA
result <- lapply(variables, function(var) {
if (!is.null(data[[var]]) && length(unique(data[[var]])) == 1) {
unique(data[[var]])
} else {
NA
}
})
# Extract metadata
ID          <- unique(data$participant)
Age         <- unique(data$age)
Gender      <- unique(data$gender)
Handedness  <- unique(data$handedness)
Date        <- unique(data$date)
# Process the subject codes, depending on the place where the data where
# collected. This is important because we used slightly different versions
# because our institutes had different ways to compensate participants.
if (place == "Muenster") {
# Get the value of vpcode from the last row, ensuring it's a single unique value
SubjectCode <- data$vpcode[which.max(seq_along(data$vpcode))]
if (is.na(SubjectCode) || SubjectCode == "" || length(SubjectCode) != 1) {
SubjectCode <- NA # Assign NA if vpcode is empty or not a single unique value
}
} else if (place == "Prague") {
#   # Get the first non-empty value of email.for.compensation
#   SubjectCode <- unique(data$email.for.compensation)
#   SubjectCode <- SubjectCode[!is.na(SubjectCode) & SubjectCode != ""]
#   if (length(SubjectCode) == 1) {
#     SubjectCode <- SubjectCode[1] # Keep as a single unique value
#   } else {
#     SubjectCode <- NA # Assign NA if there are no valid values or multiple conflicting values
#   }
# } else {
#   # Handle cases where place is neither "muenster" nor "prague"
#   SubjectCode <- NA
require(stringr)
# Define a regex for email addresses
email_regex <- "[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}"
# Extract email addresses from the data frame
SubjectCode <- unique(na.omit(str_extract(as.matrix(data), email_regex)))
}
# Ensure date is properly parsed
date_parsed <- lubridate::ymd_hms(Date)
day  <- format(date_parsed, "%Y-%m-%d")
time <- format(date_parsed, "%H:%M:%S")
# Count rows for memory and categorization tasks
nMemTrials <- sum(data$task == "memory", na.rm = TRUE)
nCatTrials <- sum(data$task == "categorization", na.rm = TRUE)
nTotTrials <- nMemTrials + nCatTrials
# Create a row summarizing the data
summary_row <- data.frame(
Index        = index,
ID           = ID,
SubjectCode  = SubjectCode,
Age          = Age,
Gender       = Gender,
Handedness   = Handedness,
Place        = place,
Day          = day,
Time         = time,
nMemTrials   = nMemTrials,
nCatTrials   = nCatTrials,
nTotTrials   = nTotTrials,
Filename     = basename(file_path), # Add the filename of the CSV
stringsAsFactors = FALSE
)
# Append the row to the summary data
summary_data <- rbind(summary_data, summary_row)
index <- index + 1
}
SubjectCode <- NA
# Loop over all files in the list
index <- 1
for (file_path in logfile_list) {
# Read the file
data <- read.csv(file_path)
# Check where the data where collected.
source(file.path(dirs$functions, "fn_check_logfile_place.R"))
place <- fn_check_logfile_place(data, vars)
source(file.path(dirs$functions, "fn_german_to_english.R"))
data <- fn_german_to_english(data, vars$column_renames)
# Makes sure that demographic variables are not empty.
variables <- c("age", "gender", "handedness", "email.for.compensation")
# Create a named list of the variables with values or NA
result <- lapply(variables, function(var) {
if (!is.null(data[[var]]) && length(unique(data[[var]])) == 1) {
unique(data[[var]])
} else {
NA
}
})
# Extract metadata
ID          <- unique(data$participant)
Age         <- unique(data$age)
Gender      <- unique(data$gender)
Handedness  <- unique(data$handedness)
Date        <- unique(data$date)
# Process the subject codes, depending on the place where the data where
# collected. This is important because we used slightly different versions
# because our institutes had different ways to compensate participants.
if (place == "Muenster") {
# Get the value of vpcode from the last row, ensuring it's a single unique value
SubjectCode <- data$vpcode[which.max(seq_along(data$vpcode))]
if (is.na(SubjectCode) || SubjectCode == "" || length(SubjectCode) != 1) {
SubjectCode <- NA # Assign NA if vpcode is empty or not a single unique value
}
} else if (place == "Prague") {
#   # Get the first non-empty value of email.for.compensation
#   SubjectCode <- unique(data$email.for.compensation)
#   SubjectCode <- SubjectCode[!is.na(SubjectCode) & SubjectCode != ""]
#   if (length(SubjectCode) == 1) {
#     SubjectCode <- SubjectCode[1] # Keep as a single unique value
#   } else {
#     SubjectCode <- NA # Assign NA if there are no valid values or multiple conflicting values
#   }
# } else {
#   # Handle cases where place is neither "muenster" nor "prague"
#   SubjectCode <- NA
require(stringr)
# Define a regex for email addresses
email_regex <- "[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}"
# Extract email addresses from the data frame
SubjectCode <- unique(na.omit(str_extract(as.matrix(data), email_regex)))
if (length(SubjectCode) == 0) {
SubjectCode <- NA
}
}
# Ensure date is properly parsed
date_parsed <- lubridate::ymd_hms(Date)
day  <- format(date_parsed, "%Y-%m-%d")
time <- format(date_parsed, "%H:%M:%S")
# Count rows for memory and categorization tasks
nMemTrials <- sum(data$task == "memory", na.rm = TRUE)
nCatTrials <- sum(data$task == "categorization", na.rm = TRUE)
nTotTrials <- nMemTrials + nCatTrials
# Create a row summarizing the data
summary_row <- data.frame(
Index        = index,
ID           = ID,
SubjectCode  = SubjectCode,
Age          = Age,
Gender       = Gender,
Handedness   = Handedness,
Place        = place,
Day          = day,
Time         = time,
nMemTrials   = nMemTrials,
nCatTrials   = nCatTrials,
nTotTrials   = nTotTrials,
Filename     = basename(file_path), # Add the filename of the CSV
stringsAsFactors = FALSE
)
# Append the row to the summary data
summary_data <- rbind(summary_data, summary_row)
index <- index + 1
}
# Get an overview of our datasets.
source(file.path(dirs$functions, "fn_logfile_overview.R"))
logfile_overview <- fn_logfile_overview(logfile_list, vars, dirs)
# Get an overview of our datasets.
source(file.path(dirs$functions, "fn_logfile_overview.R"))
logfile_overview <- fn_logfile_overview(logfile_list, vars, dirs)
# Get an overview of our datasets.
source(file.path(dirs$functions, "fn_logfile_overview.R"))
# Get an overview of our datasets.
source(file.path(dirs$functions, "fn_logfile_overview.R"))
logfile_overview <- fn_logfile_overview(logfile_list, vars, dirs)
# Get an overview of our datasets.
source(file.path(dirs$functions, "fn_logfile_overview.R"))
logfile_overview <- fn_logfile_overview(logfile_list, vars, dirs)
# Get an overview of our datasets.
source(file.path(dirs$functions, "fn_logfile_overview.R"))
logfile_overview <- fn_logfile_overview(logfile_list, vars, dirs)
logfile_overview <- fn_logfile_overview(logfile_list, vars, dirs)
# Get an overview of our datasets.
source(file.path(dirs$functions, "fn_logfile_overview.R"))
logfile_overview <- fn_logfile_overview(logfile_list, vars, dirs)
# ==============================================================================
# This script reads in the raw .csv files that were produced by Psychopy. These
# files have terrible formatting and include a lot of data that is not relevant
# for our analysis. We import these files and transform them into a data frame
# with one line per trial.
rm(list = ls(all.names = TRUE))
library(pacman)
p_load(dplyr, tidyr, stringr, readr)
# Define all variable decisions of this analysis.
vars                     <- list()
vars$exp_name            <- "scenecat_2.0"
vars$min_logfile_kbsize <- 50 # some logfiles do not include all metadata (probably because the experiment was immediately interrupted). this makes the code crash. we need to exclude these files.
# We need to code where a given dataset was collected. This is not coded in
# thePsychopy logfiles, but we can infer it from certain columns that are unique
# to the Prague/Muenster version.
vars$place_strings <- list(
"vpcode" = "Muenster",
"email.for.compensation" = "Prague",
"If.you.are.participating.for.credits.under.the.LABELS.lab..fill.in.your.email.address.." = "Prague"
)
# The German version of this experiment has some columns named in German. Wie
# need to rename these columns for consistency with the English version.
vars$column_renames <- c("Alter"       = "age",
"Geschlecht"  = "gender",
"Haendigkeit" = "handedness")
vars$columns_of_interest <- c("participant", "age", "gender", "handed", "place",
"task",
"target_cat", "cond_cat", "cat_key", "cat_corr", "cat_rt",
"cond_mem", "mem_response", "mem_perform",
"typicality", "conceptual", "perceptual",  "category", "stimulus"
)
# Where is all this stuff located? My folder structure is:
# 2024_scenecat2.0
# ├───code
# │   └───scenecat2.0_analysis
# │       └───functions
# └───data
#         ├───data_muenster
#         ├───data_muenster_pilot
#         └───data_prague
dirs <- list()
dirs$main       <- "C:/Users/nbusch/sciebo_box_projects/Projects/2024_scenecat2.0/"
dirs$functions  <- paste0(dirs$main, "/code/scenecat2.0_analysis/functions/")
dirs$data       <- paste0(dirs$main, "/data/")
# dirs$logfiles   <- paste0(dirs$data, "/data_muenster_pilot/")
# dirs$logfiles   <- paste0(dirs$data, "/data_muenster/")
dirs$logfiles   <- paste0(dirs$data, "/data_prague/")
# === Read the log files. ======================================================
# Get a list of logfiles to process.
source(file.path(dirs$functions, "fn_list_logfiles.R"))
logfile_list <- fn_list_logfiles(dirs, vars, "SceneCat_2023")
# Get an overview of our datasets.
source(file.path(dirs$functions, "fn_logfile_overview.R"))
logfile_overview <- fn_logfile_overview(logfile_list, vars, dirs)
# Read all logfiles and append the raw data.
source(file.path(dirs$functions, "fn_import_read_logfiles.R"))
raw_data <- fn_import_read_logfiles(logfile_list, vars)
# This script reads in the raw .csv files that were produced by Psychopy. These
# files have terrible formatting and include a lot of data that is not relevant
# for our analysis. We import these files and transform them into a data frame
# with one line per trial.
rm(list = ls(all.names = TRUE))
library(pacman)
p_load(dplyr, tidyr, stringr, readr)
# Define all variable decisions of this analysis.
vars                     <- list()
vars$exp_name            <- "scenecat_2.0"
vars$min_logfile_kbsize <- 50 # some logfiles do not include all metadata (probably because the experiment was immediately interrupted). this makes the code crash. we need to exclude these files.
# We need to code where a given dataset was collected. This is not coded in
# thePsychopy logfiles, but we can infer it from certain columns that are unique
# to the Prague/Muenster version.
vars$place_strings <- list(
"vpcode" = "Muenster",
"email.for.compensation" = "Prague",
"If.you.are.participating.for.credits.under.the.LABELS.lab..fill.in.your.email.address.." = "Prague"
)
# The German version of this experiment has some columns named in German. Wie
# need to rename these columns for consistency with the English version.
vars$column_renames <- c("Alter"       = "age",
"Geschlecht"  = "gender",
"Haendigkeit" = "handedness")
vars$columns_of_interest <- c("participant", "age", "gender", "handed", "place",
"task",
"target_cat", "cond_cat", "cat_key", "cat_corr", "cat_rt",
"cond_mem", "mem_response", "mem_perform",
"typicality", "conceptual", "perceptual",  "category", "stimulus"
)
# Where is all this stuff located? My folder structure is:
# 2024_scenecat2.0
# ├───code
# │   └───scenecat2.0_analysis
# │       └───functions
# └───data
#         ├───data_muenster
#         ├───data_muenster_pilot
#         └───data_prague
dirs <- list()
dirs$main       <- "C:/Users/nbusch/sciebo_box_projects/Projects/2024_scenecat2.0/"
dirs$functions  <- paste0(dirs$main, "/code/scenecat2.0_analysis/functions/")
dirs$data       <- paste0(dirs$main, "/data/")
dirs$logfiles   <- paste0(dirs$data, "/data_muenster_pilot/")
# dirs$logfiles   <- paste0(dirs$data, "/data_muenster/")
# dirs$logfiles   <- paste0(dirs$data, "/data_prague/")
# Get a list of logfiles to process.
source(file.path(dirs$functions, "fn_list_logfiles.R"))
logfile_list <- fn_list_logfiles(dirs, vars, "SceneCat_2023")
# Get an overview of our datasets.
source(file.path(dirs$functions, "fn_logfile_overview.R"))
logfile_overview <- fn_logfile_overview(logfile_list, vars, dirs)
# Read all logfiles and append the raw data.
source(file.path(dirs$functions, "fn_import_read_logfiles.R"))
raw_data <- fn_import_read_logfiles(logfile_list, vars)
# Select only the data we are interested in a bring them into a coherent format.
source(file.path(dirs$functions, "fn_import_format_logfiles.R"))
all_data <- fn_import_format_logfiles(vars, raw_data)
# This script reads in the raw .csv files that were produced by Psychopy. These
# files have terrible formatting and include a lot of data that is not relevant
# for our analysis. We import these files and transform them into a data frame
# with one line per trial.
rm(list = ls(all.names = TRUE))
library(pacman)
p_load(dplyr, tidyr, stringr, readr)
# Define all variable decisions of this analysis.
vars                     <- list()
vars$exp_name            <- "scenecat_2.0"
vars$min_logfile_kbsize <- 50 # some logfiles do not include all metadata (probably because the experiment was immediately interrupted). this makes the code crash. we need to exclude these files.
# We need to code where a given dataset was collected. This is not coded in
# thePsychopy logfiles, but we can infer it from certain columns that are unique
# to the Prague/Muenster version.
vars$place_strings <- list(
"vpcode" = "Muenster",
"email.for.compensation" = "Prague",
"If.you.are.participating.for.credits.under.the.LABELS.lab..fill.in.your.email.address.." = "Prague"
)
# The German version of this experiment has some columns named in German. Wie
# need to rename these columns for consistency with the English version.
vars$column_renames <- c("Alter"       = "age",
"Geschlecht"  = "gender",
"Haendigkeit" = "handedness")
vars$columns_of_interest <- c("participant", "age", "gender", "handed", "place",
"task",
"target_cat", "cond_cat", "cat_key", "cat_corr", "cat_rt",
"cond_mem", "mem_response", "mem_perform",
"typicality", "conceptual", "perceptual",  "category", "stimulus"
)
# Where is all this stuff located? My folder structure is:
# 2024_scenecat2.0
# ├───code
# │   └───scenecat2.0_analysis
# │       └───functions
# └───data
#         ├───data_muenster
#         ├───data_muenster_pilot
#         └───data_prague
dirs <- list()
dirs$main       <- "C:/Users/nbusch/sciebo_box_projects/Projects/2024_scenecat2.0/"
dirs$functions  <- paste0(dirs$main, "/code/scenecat2.0_analysis/functions/")
dirs$data       <- paste0(dirs$main, "/data/")
dirs$logfiles   <- paste0(dirs$data, "/data_muenster_pilot/")
# dirs$logfiles   <- paste0(dirs$data, "/data_muenster/")
# dirs$logfiles   <- paste0(dirs$data, "/data_prague/")
# Get a list of logfiles to process.
source(file.path(dirs$functions, "fn_list_logfiles.R"))
logfile_list <- fn_list_logfiles(dirs, vars, "SceneCat_2023")
# Get an overview of our datasets.
source(file.path(dirs$functions, "fn_logfile_overview.R"))
logfile_overview <- fn_logfile_overview(logfile_list, vars, dirs)
